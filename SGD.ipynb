{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c679347-ab17-4460-bf44-b8808e1a8864",
   "metadata": {},
   "source": [
    "## Задание №1\n",
    "\n",
    "Дана функция:\n",
    "\n",
    "$f(x)=0.5{x^2}−0.1\\frac{1}{e^{-x}}+0.5 cos(2x)-2$\n",
    "\n",
    "Необходимо выполнить ее аппроксимацию (восстановление) на интервале [-5, 5] моделью вида:\n",
    "\n",
    "$ a(x) = w_0 + w_1*x + w_2*x^2 + w_3 * cos(2x) + w_4 * sin(2x)$\n",
    "\n",
    "Вектор параметров \n",
    "$w = [w_0, w_1, w_2, w_3, w_4]^T$ следует искать с помощью алгоритма стохастического градиентного спуска (SGD):\n",
    "\n",
    "$w_n = w_{n-1}−η⋅ \\frac{∂L_i(w)}{∂w}$\n",
    "\n",
    "где i - случайно выбранный образ из обучающей выборки $L_i(w)$ - квадратичная функция потерь:\n",
    "\n",
    "$L_i(w) = (a(x_i, w) - y_i)^2 = (w^T * x_i - y_i)^2 $ \n",
    "\n",
    "где $ x_i = [1,x,x^2,cos(2x),sin(2x)]^T$ - вектор признаков i-го образа; \n",
    "$y_i$ - значение функции f(x) в i-й точке.\n",
    "\n",
    "Производная этой функции может быть записана в следующем векторно-матричном виде:\n",
    "\n",
    "$\\frac{∂L_i(w)}{∂w} = 2*(w^T * x_i - y_i)*x_i^T $\n",
    "\n",
    "Продолжите программу, в которой объявлена функция f(x) с именем func, заданы значения функции по оси абсцисс и ординат, а также начальные значения и параметры для алгоритма SGD\n",
    "\n",
    "На каждой итерации алгоритма SGD необходимо выбирать один случайный образ k:\n",
    "\n",
    "``k = np.random.randint(0, sz) # sz - размер выборки (массива coord_x)``\n",
    "\n",
    "из массива coord_x и на его основе формировать вектор признаков:\n",
    "\n",
    "$ x_k = [1,x,x^2,cos(2x),sin(2x)]^T$\n",
    "\n",
    "И пересчитывать значение Qe - экспоненциального скользящего среднего по формуле:\n",
    "\n",
    "$Q_e = λ * L_k(w) + (1-λ) * Q_e $\n",
    "\n",
    "Вычисленные параметры вектора w следует сохранить в виде списка или кортежа с именем ``w``. Также нужно вычислить итоговое значение среднего эмпирического риска для обученной модели по формуле:\n",
    "\n",
    "$$Q(a,X) = \\frac{1}{n}* \\sum_ {i=1}^n {L_i(w)}$$\n",
    "\n",
    "Вычисленное значение Q(a,X) сохраните в переменной ``Q``, а последнее значение $Q_e$ - в переменной ``Qe``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe05931-cee2-4f27-976e-5818c0f581c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# исходная функция, которую нужно аппроксимировать моделью a(x)\n",
    "def func(x):\n",
    "    return 0.5 * x**2 - 0.1 * 1/np.exp(-x) + 0.5 * np.cos(2*x) - 2.\n",
    "\n",
    "coord_x = np.arange(-5.0, 5.0, 0.1) # значения по оси абсцисс [-5; 5] с шагом 0.1\n",
    "coord_y = func(coord_x) # значения функции по оси ординат\n",
    "\n",
    "sz = len(coord_x)\t# количество значений функций (точек)\n",
    "eta = np.array([0.01, 0.001, 0.0001, 0.01, 0.01]) # шаг обучения для каждого параметра w0, w1, w2, w3, w4\n",
    "w = np.array([0., 0., 0., 0., 0.]) # начальные значения параметров модели\n",
    "N = 500 # число итераций алгоритма SGD\n",
    "lm = 0.02 # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
    "x_i = np.column_stack((np.ones(sz), coord_x, coord_x ** 2, np.cos(2*coord_x), np.sin(2*coord_x)))                         \n",
    "Qe = np.mean((x_i @ w.T - coord_y) ** 2) # начальное значение среднего эмпирического риска\n",
    "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
    "L = np.array([])\n",
    "\n",
    "for i in range(N):\n",
    "    k = np.random.randint(0, sz-1)\n",
    "\n",
    "    x = x_i[k]\n",
    "\n",
    "    l = (x @ w.T - coord_y[k]) ** 2\n",
    "    L = np.append(L,l)\n",
    "\n",
    "    dl = 2 * (x @ w.T - coord_y[k]) * x.T\n",
    "\n",
    "    w -= eta * dl\n",
    "\n",
    "    Qe = lm * l + (1-lm) * Qe\n",
    "\n",
    "Q = np.mean((x_i @ w.T - coord_y) ** 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb22869-f35a-45e9-9653-639760f2bc7b",
   "metadata": {},
   "source": [
    "## Задание №2\n",
    "\n",
    "Дана функция:\n",
    "\n",
    "$f(x)=0.5x + 0.2x^2 - 0.05x^3 + 0.2sin(4x) - 2.5 $\n",
    "\n",
    "Необходимо выполнить ее аппроксимацию (восстановление) на интервале [-4, 6] моделью вида:\n",
    "\n",
    "$ a(x) = w_0 + w_1*x + w_2*x^2 + w_3 * x^3$\n",
    "\n",
    "Вектор параметров $w = [w_0,w_1,w_2,w_3]^T$ следует искать с помощью алгоритма стохастического градиентного спуска (SGD) с квадратичной функцией потерь:\n",
    "\n",
    "$w_n = w_{n-1}−η⋅ \\frac{∂Qk(w)}{∂w}$\n",
    "\n",
    "где k - случайно выбранный образ из обучающей выборки; Qk(w) - усеченный эмпирический риск:\n",
    "\n",
    "$$Qk(w) = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} (a(x_i,w) - y_i)^2 = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} (w^T*x_i - y_i)^2 $$\n",
    "\n",
    "где $x_i = [1,x,x^2,x^3]^T $ - вектор признаков i-го образа; $y_i$ - значение функции f(x) в i-й точке.\n",
    "\n",
    "Производная усеченного показателя качества может быть записана в следующем векторно-матричном виде:\n",
    "\n",
    "$$\\frac{∂Qk(w)}{∂w} = \\frac{2}{K} \\sum_ {i=k}^{k+K-1} (w^T * x_i - y_i) * x_i^T $$\n",
    "\n",
    "То есть, псевдоградиент вычисляется не по одному образу k, а по образам в диапазоне [k; k+K) (всего K образов).\n",
    "\n",
    "Продолжите программу, в которой объявлена функция f(x) с именем func, заданы значения функции по оси абсцисс и ординат, а также начальные значения и параметры для алгоритма SGD\n",
    "\n",
    "На каждой итерации алгоритма SGD необходимо выбирать начальное значение k командой:\n",
    "\n",
    "``k = np.random.randint(0, sz-batch_size) # sz - размер выборки (массива coord_x)``\n",
    "\n",
    "и из массива coord_x формировать векторы признаков:\n",
    "\n",
    "$x_i = [1,x,x^2,x^3]^T$\n",
    "\n",
    "для вычисления псевдоградиента Qk по образам от k до k + batch_size (команда ``range(k, k+batch_size)``).\n",
    "\n",
    "Также на каждой итерации нужно пересчитывать значение Qe - экспоненциального скользящего среднего по формуле:\n",
    "\n",
    "$Q_e = λ * Qk(w) + (1-λ) * Q_e $\n",
    "\n",
    "Вычисленные параметры вектора w следует сохранить в виде списка или кортежа с именем ``w``. Также нужно вычислить итоговое значение среднего эмпирического риска для обученной модели по формуле:\n",
    "\n",
    "$$Q(a,X) = \\frac{1}{n}* \\sum_ {i=1}^n {L_i(w)}$$\n",
    "\n",
    "Вычисленное значение Q(a,X) сохраните в переменной ``Q``, а последнее значение $Q_e$ - в переменной ``Qe``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a1891-1988-448b-a86d-1fda3213d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# исходная функция, которую нужно аппроксимировать моделью a(x)\n",
    "def func(x):\n",
    "    return 0.5 * x + 0.2 * x ** 2 - 0.05 * x ** 3 + 0.2 * np.sin(4 * x) - 2.5\n",
    "\n",
    "\n",
    "coord_x = np.arange(-4.0, 6.0, 0.1) # значения по оси абсцисс [-4; 6] с шагом 0.1\n",
    "coord_y = func(coord_x) # значения функции по оси ординат\n",
    "\n",
    "sz = len(coord_x)\t# количество значений функций (точек)\n",
    "eta = np.array([0.1, 0.01, 0.001, 0.0001]) # шаг обучения для каждого параметра w0, w1, w2, w3\n",
    "w = np.array([0., 0., 0., 0.]) # начальные значения параметров модели\n",
    "N = 500 # число итераций алгоритма SGD\n",
    "lm = 0.02 # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
    "batch_size = 50 # размер мини-батча (величина K = 50)\n",
    "x_i = np.column_stack((np.ones(sz), coord_x, coord_x ** 2, coord_x ** 3))\n",
    "\n",
    "Qe = np.mean((x_i @ w.T - coord_y) ** 2)\n",
    "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
    "\n",
    "for _ in range(N):\n",
    "    k = np.random.randint(0, sz-batch_size-1)\n",
    "    x_batch = x_i[k:k+batch_size]\n",
    "    y_batch = coord_y[k:k+batch_size]\n",
    "\n",
    "    y_pred = x_batch @ w.T\n",
    "\n",
    "    error = y_pred - y_batch\n",
    "    Qk = np.mean(error ** 2)  # усечённый эмпирический риск для мини-батча\n",
    "\n",
    "    # Обновление скользящего экспоненциального среднего эмпирического риска\n",
    "    Qe = lm * Qk + (1 - lm) * Qe\n",
    "\n",
    "    er_col = error.reshape(batch_size, 1)\n",
    "    # Вычисление псевдоградиента\n",
    "    grad = 2 * np.mean(er_col * x_batch, axis=0)\n",
    "    \n",
    "    # Обновление параметров модели\n",
    "    w -= eta * grad\n",
    "\n",
    "\n",
    "Q = np.mean((x_i @ w.T - coord_y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4567f0-7b97-43e4-a2d9-a44a394cca8d",
   "metadata": {},
   "source": [
    "## Задание №3\n",
    "\n",
    "Дана функция:\n",
    "\n",
    "$f(x) = -0.7x -0.2x^2 +0.05x^3 -0.2cos(3x) + 2$\n",
    "\n",
    "Необходимо выполнить ее аппроксимацию (восстановление) на интервале [-4, 6] моделью вида:\n",
    "\n",
    "$ a(x) = w_0 + w_1*x + w_2*x^2 + w_3 * x^3$\n",
    "\n",
    "Вектор параметров $w = [w_0,w_1,w_2,w_3]^T $ следует искать с помощью алгоритма стохастического градиентного спуска (SGD) с оптимизатором импульсов Нестерова:\n",
    "\n",
    "$v=γ⋅v+(1−γ)⋅η⋅\\frac{∂Qk(w−γ⋅v)}{∂w} $\n",
    "\n",
    "$w_n = w_{n-1} - v $\n",
    "\n",
    "где k - случайно выбранный образ из обучающей выборки; Qk(w) - усеченный эмпирический риск:\n",
    "\n",
    "$$Qk(w) = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} (a(x_i,w) - y_i)^2 = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} (w^T*x_i - y_i)^2 $$\n",
    "\n",
    "где $x_i = [1,x,x^2,x^3]^T $ - вектор признаков i-го образа; $y_i$ - значение функции f(x) в i-й точке.\n",
    "\n",
    "Производная усеченного показателя качества может быть записана в следующем векторно-матричном виде:\n",
    "\n",
    "$$\\frac{∂Qk(w−γ⋅v)}{∂w} = \\frac{2}{K} \\sum_ {i=k}^{k+K-1} ((w−γ⋅v)^T * x_i - y_i) * x_i^T $$\n",
    "\n",
    "То есть, псевдоградиент вычисляется не по одному образу k, а по образам в диапазоне [k; k+K) (всего K образов).\n",
    "\n",
    "Продолжите программу, в которой объявлена функция f(x) с именем func, заданы значения функции по оси абсцисс и ординат, а также начальные значения и параметры для алгоритма SGD с импульсами Нестерова\n",
    "\n",
    "На каждой итерации алгоритма SGD необходимо выбирать начальное значение k командой:\n",
    "\n",
    "``k = np.random.randint(0, sz-batch_size-1) # sz - размер выборки (массива coord_x)``\n",
    "\n",
    "и из массива coord_x формировать векторы признаков:\n",
    "$x_i = [1,x,x^2,x^3]^T$\n",
    "\n",
    "для вычисления псевдоградиента Qk по образам от k до k + batch_size (команда ``range(k, k+batch_size)``)\n",
    "\n",
    "Также на каждой итерации нужно пересчитывать значение Qe экспоненциального скользящего среднего по формуле:\n",
    "\n",
    "$Q_e = λ * Qk(w) + (1-λ) * Q_e $\n",
    "\n",
    "Вычисленные параметры вектора w следует сохранить в виде списка или кортежа с именем w. Также нужно вычислить итоговое значение среднего эмпирического риска для обученной модели по формуле:\n",
    "\n",
    "$$Q(a,X) = \\frac{1}{n}* \\sum_ {i=1}^n {L_i(w)}$$\n",
    "\n",
    "Вычисленное значение Q(a,X) сохраните в переменной ``Q``, а последнее значение Qe - в переменной ``Qe``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853f809-f0eb-4f33-b5c6-959f578138e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# исходная функция, которую нужно аппроксимировать моделью a(x)\n",
    "def func(x):\n",
    "    return -0.7 * x - 0.2 * x ** 2 + 0.05 * x ** 3 - 0.2 * np.cos(3 * x) + 2\n",
    "\n",
    "def a(x,w):\n",
    "    return x @ w.T\n",
    "\n",
    "coord_x = np.arange(-4.0, 6.0, 0.1) # значения по оси абсцисс [-4; 6] с шагом 0.1\n",
    "coord_y = func(coord_x) # значения функции по оси ординат\n",
    "\n",
    "sz = len(coord_x)\t# количество значений функций (точек)\n",
    "eta = np.array([0.1, 0.01, 0.001, 0.0001]) # шаг обучения для каждого параметра w0, w1, w2, w3\n",
    "w = np.array([0., 0., 0., 0.]) # начальные значения параметров модели\n",
    "N = 500 # число итераций алгоритма SGD\n",
    "lm = 0.02 # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
    "batch_size = 20 # размер мини-батча (величина K = 20)\n",
    "gamma = 0.8 # коэффициент гамма для вычисления импульсов Нестерова\n",
    "v = np.zeros(len(w))  # начальное значение [0, 0, 0, 0]\n",
    "x = np.array([(1, xx, xx ** 2, xx ** 3) for xx in coord_x])\n",
    "\n",
    "Qe = 0\n",
    "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
    "\n",
    "for _ in range(N):\n",
    "    k = np.random.randint(0, sz-batch_size-1) # sz - размер выборки (массива coord_x)\n",
    "    x_batch = x[k:k+batch_size]\n",
    "    y_batch = coord_y[k:k+batch_size]\n",
    "\n",
    "    Qk = np.mean([(a(x[i], w) - coord_y[i]) ** 2 for i in range(k,k+batch_size)])\n",
    "\n",
    "    dQk = 2 * np.mean([((w - gamma * v).T @ x[i] - coord_y[i]) * x[i].T for i in range(k, k+batch_size)], axis=0)\n",
    "\n",
    "    v = gamma * v + (1-gamma) * eta * dQk\n",
    "\n",
    "    w -= v\n",
    "\n",
    "    Qe = lm * Qk + (1-lm) * Qe\n",
    "\n",
    "Q = np.mean((a(x,w) - coord_y) ** 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
