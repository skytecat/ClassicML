{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c679347-ab17-4460-bf44-b8808e1a8864",
   "metadata": {},
   "source": [
    "## Задание №1\n",
    "\n",
    "Дана функция:\n",
    "\n",
    "$f(x)=0.5{x^2}−0.1\\frac{1}{e^{-x}}+0.5 cos(2x)-2$\n",
    "\n",
    "Необходимо выполнить ее аппроксимацию (восстановление) на интервале [-5, 5] моделью вида:\n",
    "\n",
    "$ a(x) = w_0 + w_1*x + w_2*x^2 + w_3 * cos(2x) + w_4 * sin(2x)$\n",
    "\n",
    "Вектор параметров \n",
    "$w = [w_0, w_1, w_2, w_3, w_4]^T$ следует искать с помощью алгоритма стохастического градиентного спуска (SGD):\n",
    "\n",
    "$w_n = w_{n-1}−η⋅ \\frac{∂L_i(w)}{∂w}$\n",
    "\n",
    "где i - случайно выбранный образ из обучающей выборки $L_i(w)$ - квадратичная функция потерь:\n",
    "\n",
    "$L_i(w) = (a(x_i, w) - y_i)^2 = (w^T * x_i - y_i)^2 $ \n",
    "\n",
    "где $ x_i = [1,x,x^2,cos(2x),sin(2x)]^T$ - вектор признаков i-го образа; \n",
    "$y_i$ - значение функции f(x) в i-й точке.\n",
    "\n",
    "Производная этой функции может быть записана в следующем векторно-матричном виде:\n",
    "\n",
    "$\\frac{∂L_i(w)}{∂w} = 2*(w^T * x_i - y_i)*x_i^T $\n",
    "\n",
    "Продолжите программу, в которой объявлена функция f(x) с именем func, заданы значения функции по оси абсцисс и ординат, а также начальные значения и параметры для алгоритма SGD\n",
    "\n",
    "На каждой итерации алгоритма SGD необходимо выбирать один случайный образ k:\n",
    "\n",
    "``k = np.random.randint(0, sz) # sz - размер выборки (массива coord_x)``\n",
    "\n",
    "из массива coord_x и на его основе формировать вектор признаков:\n",
    "\n",
    "$ x_k = [1,x,x^2,cos(2x),sin(2x)]^T$\n",
    "\n",
    "И пересчитывать значение Qe - экспоненциального скользящего среднего по формуле:\n",
    "\n",
    "$Q_e = λ * L_k(w) + (1-λ) * Q_e $\n",
    "\n",
    "Вычисленные параметры вектора w следует сохранить в виде списка или кортежа с именем ``w``. Также нужно вычислить итоговое значение среднего эмпирического риска для обученной модели по формуле:\n",
    "\n",
    "$$Q(a,X) = \\frac{1}{n}* \\sum_ {i=1}^n {L_i(w)}$$\n",
    "\n",
    "Вычисленное значение Q(a,X) сохраните в переменной ``Q``, а последнее значение $Q_e$ - в переменной ``Qe``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe05931-cee2-4f27-976e-5818c0f581c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# исходная функция, которую нужно аппроксимировать моделью a(x)\n",
    "def func(x):\n",
    "    return 0.5 * x**2 - 0.1 * 1/np.exp(-x) + 0.5 * np.cos(2*x) - 2.\n",
    "\n",
    "coord_x = np.arange(-5.0, 5.0, 0.1) # значения по оси абсцисс [-5; 5] с шагом 0.1\n",
    "coord_y = func(coord_x) # значения функции по оси ординат\n",
    "\n",
    "sz = len(coord_x)\t# количество значений функций (точек)\n",
    "eta = np.array([0.01, 0.001, 0.0001, 0.01, 0.01]) # шаг обучения для каждого параметра w0, w1, w2, w3, w4\n",
    "w = np.array([0., 0., 0., 0., 0.]) # начальные значения параметров модели\n",
    "N = 500 # число итераций алгоритма SGD\n",
    "lm = 0.02 # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
    "x_i = np.column_stack((np.ones(sz), coord_x, coord_x ** 2, np.cos(2*coord_x), np.sin(2*coord_x)))                         \n",
    "Qe = np.mean((x_i @ w.T - coord_y) ** 2) # начальное значение среднего эмпирического риска\n",
    "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
    "L = np.array([])\n",
    "\n",
    "for i in range(N):\n",
    "    k = np.random.randint(0, sz-1)\n",
    "\n",
    "    x = x_i[k]\n",
    "\n",
    "    l = (x @ w.T - coord_y[k]) ** 2\n",
    "    L = np.append(L,l)\n",
    "\n",
    "    dl = 2 * (x @ w.T - coord_y[k]) * x.T\n",
    "\n",
    "    w -= eta * dl\n",
    "\n",
    "    Qe = lm * l + (1-lm) * Qe\n",
    "\n",
    "Q = np.mean((x_i @ w.T - coord_y) ** 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb22869-f35a-45e9-9653-639760f2bc7b",
   "metadata": {},
   "source": [
    "## Задание №2\n",
    "\n",
    "Дана функция:\n",
    "\n",
    "$f(x)=0.5x + 0.2x^2 - 0.05x^3 + 0.2sin(4x) - 2.5 $\n",
    "\n",
    "Необходимо выполнить ее аппроксимацию (восстановление) на интервале [-4, 6] моделью вида:\n",
    "\n",
    "$ a(x) = w_0 + w_1*x + w_2*x^2 + w_3 * x^3$\n",
    "\n",
    "Вектор параметров $w = [w_0,w_1,w_2,w_3]^T$ следует искать с помощью алгоритма стохастического градиентного спуска (SGD) с квадратичной функцией потерь:\n",
    "\n",
    "$w_n = w_{n-1}−η⋅ \\frac{∂Qk(w)}{∂w}$\n",
    "\n",
    "где k - случайно выбранный образ из обучающей выборки; Qk(w) - усеченный эмпирический риск:\n",
    "\n",
    "$$Qk(w) = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} (a(x_i,w) - y_i)^2 = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} (w^T*x_i - y_i)^2 $$\n",
    "\n",
    "где $x_i = [1,x,x^2,x^3]^T $ - вектор признаков i-го образа; $y_i$ - значение функции f(x) в i-й точке.\n",
    "\n",
    "Производная усеченного показателя качества может быть записана в следующем векторно-матричном виде:\n",
    "\n",
    "$$\\frac{∂Qk(w)}{∂w} = \\frac{2}{K} \\sum_ {i=k}^{k+K-1} (w^T * x_i - y_i) * x_i^T $$\n",
    "\n",
    "То есть, псевдоградиент вычисляется не по одному образу k, а по образам в диапазоне [k; k+K) (всего K образов).\n",
    "\n",
    "Продолжите программу, в которой объявлена функция f(x) с именем func, заданы значения функции по оси абсцисс и ординат, а также начальные значения и параметры для алгоритма SGD\n",
    "\n",
    "На каждой итерации алгоритма SGD необходимо выбирать начальное значение k командой:\n",
    "\n",
    "``k = np.random.randint(0, sz-batch_size) # sz - размер выборки (массива coord_x)``\n",
    "\n",
    "и из массива coord_x формировать векторы признаков:\n",
    "\n",
    "$x_i = [1,x,x^2,x^3]^T$\n",
    "\n",
    "для вычисления псевдоградиента Qk по образам от k до k + batch_size (команда ``range(k, k+batch_size)``).\n",
    "\n",
    "Также на каждой итерации нужно пересчитывать значение Qe - экспоненциального скользящего среднего по формуле:\n",
    "\n",
    "$Q_e = λ * Qk(w) + (1-λ) * Q_e $\n",
    "\n",
    "Вычисленные параметры вектора w следует сохранить в виде списка или кортежа с именем ``w``. Также нужно вычислить итоговое значение среднего эмпирического риска для обученной модели по формуле:\n",
    "\n",
    "$$Q(a,X) = \\frac{1}{n}* \\sum_ {i=1}^n {L_i(w)}$$\n",
    "\n",
    "Вычисленное значение Q(a,X) сохраните в переменной ``Q``, а последнее значение $Q_e$ - в переменной ``Qe``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a1891-1988-448b-a86d-1fda3213d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# исходная функция, которую нужно аппроксимировать моделью a(x)\n",
    "def func(x):\n",
    "    return 0.5 * x + 0.2 * x ** 2 - 0.05 * x ** 3 + 0.2 * np.sin(4 * x) - 2.5\n",
    "\n",
    "\n",
    "coord_x = np.arange(-4.0, 6.0, 0.1) # значения по оси абсцисс [-4; 6] с шагом 0.1\n",
    "coord_y = func(coord_x) # значения функции по оси ординат\n",
    "\n",
    "sz = len(coord_x)\t# количество значений функций (точек)\n",
    "eta = np.array([0.1, 0.01, 0.001, 0.0001]) # шаг обучения для каждого параметра w0, w1, w2, w3\n",
    "w = np.array([0., 0., 0., 0.]) # начальные значения параметров модели\n",
    "N = 500 # число итераций алгоритма SGD\n",
    "lm = 0.02 # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
    "batch_size = 50 # размер мини-батча (величина K = 50)\n",
    "x_i = np.column_stack((np.ones(sz), coord_x, coord_x ** 2, coord_x ** 3))\n",
    "\n",
    "Qe = np.mean((x_i @ w.T - coord_y) ** 2)\n",
    "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
    "\n",
    "for _ in range(N):\n",
    "    k = np.random.randint(0, sz-batch_size-1)\n",
    "    x_batch = x_i[k:k+batch_size]\n",
    "    y_batch = coord_y[k:k+batch_size]\n",
    "\n",
    "    y_pred = x_batch @ w.T\n",
    "\n",
    "    error = y_pred - y_batch\n",
    "    Qk = np.mean(error ** 2)  # усечённый эмпирический риск для мини-батча\n",
    "\n",
    "    # Обновление скользящего экспоненциального среднего эмпирического риска\n",
    "    Qe = lm * Qk + (1 - lm) * Qe\n",
    "\n",
    "    er_col = error.reshape(batch_size, 1)\n",
    "    # Вычисление псевдоградиента\n",
    "    grad = 2 * np.mean(er_col * x_batch, axis=0)\n",
    "    \n",
    "    # Обновление параметров модели\n",
    "    w -= eta * grad\n",
    "\n",
    "\n",
    "Q = np.mean((x_i @ w.T - coord_y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4567f0-7b97-43e4-a2d9-a44a394cca8d",
   "metadata": {},
   "source": [
    "## Задание №3\n",
    "\n",
    "Дана функция:\n",
    "\n",
    "$f(x) = -0.7x -0.2x^2 +0.05x^3 -0.2cos(3x) + 2$\n",
    "\n",
    "Необходимо выполнить ее аппроксимацию (восстановление) на интервале [-4, 6] моделью вида:\n",
    "\n",
    "$ a(x) = w_0 + w_1*x + w_2*x^2 + w_3 * x^3$\n",
    "\n",
    "Вектор параметров $w = [w_0,w_1,w_2,w_3]^T $ следует искать с помощью алгоритма стохастического градиентного спуска (SGD) с оптимизатором импульсов Нестерова:\n",
    "\n",
    "$v=γ⋅v+(1−γ)⋅η⋅\\frac{∂Qk(w−γ⋅v)}{∂w} $\n",
    "\n",
    "$w_n = w_{n-1} - v $\n",
    "\n",
    "где k - случайно выбранный образ из обучающей выборки; Qk(w) - усеченный эмпирический риск:\n",
    "\n",
    "$$Qk(w) = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} (a(x_i,w) - y_i)^2 = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} (w^T*x_i - y_i)^2 $$\n",
    "\n",
    "где $x_i = [1,x,x^2,x^3]^T $ - вектор признаков i-го образа; $y_i$ - значение функции f(x) в i-й точке.\n",
    "\n",
    "Производная усеченного показателя качества может быть записана в следующем векторно-матричном виде:\n",
    "\n",
    "$$\\frac{∂Qk(w−γ⋅v)}{∂w} = \\frac{2}{K} \\sum_ {i=k}^{k+K-1} ((w−γ⋅v)^T * x_i - y_i) * x_i^T $$\n",
    "\n",
    "То есть, псевдоградиент вычисляется не по одному образу k, а по образам в диапазоне [k; k+K) (всего K образов).\n",
    "\n",
    "Продолжите программу, в которой объявлена функция f(x) с именем func, заданы значения функции по оси абсцисс и ординат, а также начальные значения и параметры для алгоритма SGD с импульсами Нестерова\n",
    "\n",
    "На каждой итерации алгоритма SGD необходимо выбирать начальное значение k командой:\n",
    "\n",
    "``k = np.random.randint(0, sz-batch_size-1) # sz - размер выборки (массива coord_x)``\n",
    "\n",
    "и из массива coord_x формировать векторы признаков:\n",
    "$x_i = [1,x,x^2,x^3]^T$\n",
    "\n",
    "для вычисления псевдоградиента Qk по образам от k до k + batch_size (команда ``range(k, k+batch_size)``)\n",
    "\n",
    "Также на каждой итерации нужно пересчитывать значение Qe экспоненциального скользящего среднего по формуле:\n",
    "\n",
    "$Q_e = λ * Qk(w) + (1-λ) * Q_e $\n",
    "\n",
    "Вычисленные параметры вектора w следует сохранить в виде списка или кортежа с именем w. Также нужно вычислить итоговое значение среднего эмпирического риска для обученной модели по формуле:\n",
    "\n",
    "$$Q(a,X) = \\frac{1}{n}* \\sum_ {i=1}^n {L_i(w)}$$\n",
    "\n",
    "Вычисленное значение Q(a,X) сохраните в переменной ``Q``, а последнее значение Qe - в переменной ``Qe``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853f809-f0eb-4f33-b5c6-959f578138e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# исходная функция, которую нужно аппроксимировать моделью a(x)\n",
    "def func(x):\n",
    "    return -0.7 * x - 0.2 * x ** 2 + 0.05 * x ** 3 - 0.2 * np.cos(3 * x) + 2\n",
    "\n",
    "def a(x,w):\n",
    "    return x @ w.T\n",
    "\n",
    "coord_x = np.arange(-4.0, 6.0, 0.1) # значения по оси абсцисс [-4; 6] с шагом 0.1\n",
    "coord_y = func(coord_x) # значения функции по оси ординат\n",
    "\n",
    "sz = len(coord_x)\t# количество значений функций (точек)\n",
    "eta = np.array([0.1, 0.01, 0.001, 0.0001]) # шаг обучения для каждого параметра w0, w1, w2, w3\n",
    "w = np.array([0., 0., 0., 0.]) # начальные значения параметров модели\n",
    "N = 500 # число итераций алгоритма SGD\n",
    "lm = 0.02 # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
    "batch_size = 20 # размер мини-батча (величина K = 20)\n",
    "gamma = 0.8 # коэффициент гамма для вычисления импульсов Нестерова\n",
    "v = np.zeros(len(w))  # начальное значение [0, 0, 0, 0]\n",
    "x = np.array([(1, xx, xx ** 2, xx ** 3) for xx in coord_x])\n",
    "\n",
    "Qe = 0\n",
    "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
    "\n",
    "for _ in range(N):\n",
    "    k = np.random.randint(0, sz-batch_size-1) # sz - размер выборки (массива coord_x)\n",
    "    x_batch = x[k:k+batch_size]\n",
    "    y_batch = coord_y[k:k+batch_size]\n",
    "\n",
    "    Qk = np.mean([(a(x[i], w) - coord_y[i]) ** 2 for i in range(k,k+batch_size)])\n",
    "\n",
    "    dQk = 2 * np.mean([((w - gamma * v).T @ x[i] - coord_y[i]) * x[i].T for i in range(k, k+batch_size)], axis=0)\n",
    "\n",
    "    v = gamma * v + (1-gamma) * eta * dQk\n",
    "\n",
    "    w -= v\n",
    "\n",
    "    Qe = lm * Qk + (1-lm) * Qe\n",
    "\n",
    "Q = np.mean((a(x,w) - coord_y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2d5ff-49bf-4008-aa36-a7e295bd41de",
   "metadata": {},
   "source": [
    "## Задание №4\n",
    "\n",
    "Дана следующая обучающая выборка для задачи бинарной классификации и визуально образы этой выборки распределены в двумерном пространстве признаков следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6a48394-e41c-49c2-ba39-99223ae57dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.3, 2.3)\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# data_x = [(5.3, 2.3), (5.7, 2.5), (4.0, 1.0), (5.6, 2.4), (4.5, 1.5), (5.4, 2.3), (4.8, 1.8), (4.5, 1.5), (5.1, 1.5), (6.1, 2.3), (5.1, 1.9), (4.0, 1.2), (5.2, 2.0), (3.9, 1.4), (4.2, 1.2), (4.7, 1.5), (4.8, 1.8), (3.6, 1.3), (4.6, 1.4), (4.5, 1.7), (3.0, 1.1), (4.3, 1.3), (4.5, 1.3), (5.5, 2.1), (3.5, 1.0), (5.6, 2.2), (4.2, 1.5), (5.8, 1.8), (5.5, 1.8), (5.7, 2.3), (6.4, 2.0), (5.0, 1.7), (6.7, 2.0), (4.0, 1.3), (4.4, 1.4), (4.5, 1.5), (5.6, 2.4), (5.8, 1.6), (4.6, 1.3), (4.1, 1.3), (5.1, 2.3), (5.2, 2.3), (5.6, 1.4), (5.1, 1.8), (4.9, 1.5), (6.7, 2.2), (4.4, 1.3), (3.9, 1.1), (6.3, 1.8), (6.0, 1.8), (4.5, 1.6), (6.6, 2.1), (4.1, 1.3), (4.5, 1.5), (6.1, 2.5), (4.1, 1.0), (4.4, 1.2), (5.4, 2.1), (5.0, 1.5), (5.0, 2.0), (4.9, 1.5), (5.9, 2.1), (4.3, 1.3), (4.0, 1.3), (4.9, 2.0), (4.9, 1.8), (4.0, 1.3), (5.5, 1.8), (3.7, 1.0), (6.9, 2.3), (5.7, 2.1), (5.3, 1.9), (4.4, 1.4), (5.6, 1.8), (3.3, 1.0), (4.8, 1.8), (6.0, 2.5), (5.9, 2.3), (4.9, 1.8), (3.3, 1.0), (3.9, 1.2), (5.6, 2.1), (5.8, 2.2), (3.8, 1.1), (3.5, 1.0), (4.5, 1.5), (5.1, 1.9), (4.7, 1.4), (5.1, 1.6), (5.1, 2.0), (4.8, 1.4), (5.0, 1.9), (5.1, 2.4), (4.6, 1.5), (6.1, 1.9), (4.7, 1.6), (4.7, 1.4), (4.7, 1.2), (4.2, 1.3), (4.2, 1.3)]\n",
    "# data_y = [1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, -1]\n",
    "\n",
    "# cols = ['blue', 'red']\n",
    "\n",
    "# x1 = data_x[data_y==1]\n",
    "# y1 = data_x[data_y==1][1]\n",
    "\n",
    "# print(x1)\n",
    "\n",
    "# for k in np.unique(data_y):\n",
    "# plt.plot(data_x[data_y==1][0], data_x[data_y==1][1], 'o',label='Класс {}'.format(1), color=cols[0])\n",
    "    \n",
    "# plt.legend(loc='best')\n",
    "# plt.show()\n",
    "# plt.plot(data_x[data_y==-1][0],2.5,'o',label='1',color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6de9d-3f6d-4c5b-993c-59843b1e28d8",
   "metadata": {},
   "source": [
    "Вам необходимо продолжить программу для вычисления вектора параметров:\n",
    "\n",
    "$w = [w_0,w_1,w_2]^T $\n",
    "\n",
    "которым описывается разделяющая линия в соответствии с выражением:\n",
    "\n",
    "$w_1*x_1 + w_2*x_2 + w_0 = 0 $\n",
    "\n",
    "Вектор параметров w следует искать с помощью алгоритма стохастического градиентного спуска (SGD) с оптимизатором RMSProp:\n",
    "\n",
    "$G = α*G+(1−α)*\\frac{∂Qk(w)}{∂w}*\\frac{∂Qk(w)}{∂w}$\n",
    "\n",
    "$w_n = w_{n-1}-η * \\frac{∂Qk(w)/∂w}{\\sqrt G+ϵ}$\n",
    "\n",
    "где k - случайно выбранный образ из обучающей выборки; Qk(w) - усеченный эмпирический риск:\n",
    "\n",
    "$$Qk(w) = \\frac{1}{K}* \\sum_ {i=k}^{k+K-1} {L_i(w)}$$\n",
    "\n",
    "То есть, псевдоградиент вычисляется не по одному образу k, а по образам в диапазоне [k; k+K) (всего K образов).\n",
    "\n",
    "В качестве функции потерь следует выбрать логарифмическую функцию:\n",
    "\n",
    "$L_i(w) = log_2(1+e^{-M_i}) = log_2(1+e^{-w^T*x_i*y_i}) $\n",
    "\n",
    "Ее производная по вектору параметров w, равна:\n",
    "\n",
    "$$\\frac{∂L_i(w)}{∂w} = - \\frac{e^{-w^T*x_i*y_i} * x_i^T * y_i}{(1+e^{-w^T*x_i*y_i})*ln(2)}$$\n",
    "\n",
    "Продолжите программу, в которой сформирована обучающая выборка, заданы функции потерь и ее производной, а также начальные значения и параметры для алгоритма SGD\n",
    "\n",
    "На каждой итерации алгоритма SGD необходимо выбирать начальное значение k командой:\n",
    "\n",
    "``k = np.random.randint(0, n_train-batch_size-1) # n_train - размер выборки (массива x_train)``\n",
    "\n",
    "и из массива x_train выбирать векторы признаков ${x_i} $для вычисления псевдоградиента по образам от k до k + batch_size (команда ``range(k, k+batch_size)``).\n",
    "\n",
    "Также на каждой итерации нужно пересчитывать значение Qe экспоненциального скользящего среднего по формуле:\n",
    "\n",
    "$Q_e = λ * Qk(w) + (1-λ) * Q_e $\n",
    "\n",
    "Вычисленные параметры вектора w следует сохранить в виде списка или кортежа с именем ``w``. Также нужно вычислить итоговое значение среднего эмпирического риска для обученной модели по формуле:\n",
    "\n",
    "$$Q(a,X) = \\frac{1}{n}* \\sum_ {i=1}^n [M_i < 0] $$\n",
    "\n",
    "Вычисленное значение Q(a,X) сохраните в переменной ``Q``, а последнее значение Qe - в переменной ``Qe``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb425a6-5a38-4045-91da-33b71f0ec391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# логарифмическая функция потерь\n",
    "def loss(w, x, y):\n",
    "    M = np.dot(w, x) * y\n",
    "    return np.log2(1 + np.exp(-M))\n",
    "\n",
    "\n",
    "# производная логарифмической функции потерь по вектору w\n",
    "def df(w, x, y):\n",
    "    M = np.dot(w, x) * y\n",
    "    return -(np.exp(-M) * x.T * y) / ((1 + np.exp(-M)) * np.log(2))\n",
    "\n",
    "\n",
    "data_x = [(5.3, 2.3), (5.7, 2.5), (4.0, 1.0), (5.6, 2.4), (4.5, 1.5), (5.4, 2.3), (4.8, 1.8), (4.5, 1.5), (5.1, 1.5), (6.1, 2.3), (5.1, 1.9), (4.0, 1.2), (5.2, 2.0), (3.9, 1.4), (4.2, 1.2), (4.7, 1.5), (4.8, 1.8), (3.6, 1.3), (4.6, 1.4), (4.5, 1.7), (3.0, 1.1), (4.3, 1.3), (4.5, 1.3), (5.5, 2.1), (3.5, 1.0), (5.6, 2.2), (4.2, 1.5), (5.8, 1.8), (5.5, 1.8), (5.7, 2.3), (6.4, 2.0), (5.0, 1.7), (6.7, 2.0), (4.0, 1.3), (4.4, 1.4), (4.5, 1.5), (5.6, 2.4), (5.8, 1.6), (4.6, 1.3), (4.1, 1.3), (5.1, 2.3), (5.2, 2.3), (5.6, 1.4), (5.1, 1.8), (4.9, 1.5), (6.7, 2.2), (4.4, 1.3), (3.9, 1.1), (6.3, 1.8), (6.0, 1.8), (4.5, 1.6), (6.6, 2.1), (4.1, 1.3), (4.5, 1.5), (6.1, 2.5), (4.1, 1.0), (4.4, 1.2), (5.4, 2.1), (5.0, 1.5), (5.0, 2.0), (4.9, 1.5), (5.9, 2.1), (4.3, 1.3), (4.0, 1.3), (4.9, 2.0), (4.9, 1.8), (4.0, 1.3), (5.5, 1.8), (3.7, 1.0), (6.9, 2.3), (5.7, 2.1), (5.3, 1.9), (4.4, 1.4), (5.6, 1.8), (3.3, 1.0), (4.8, 1.8), (6.0, 2.5), (5.9, 2.3), (4.9, 1.8), (3.3, 1.0), (3.9, 1.2), (5.6, 2.1), (5.8, 2.2), (3.8, 1.1), (3.5, 1.0), (4.5, 1.5), (5.1, 1.9), (4.7, 1.4), (5.1, 1.6), (5.1, 2.0), (4.8, 1.4), (5.0, 1.9), (5.1, 2.4), (4.6, 1.5), (6.1, 1.9), (4.7, 1.6), (4.7, 1.4), (4.7, 1.2), (4.2, 1.3), (4.2, 1.3)]\n",
    "data_y = [1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, -1]\n",
    "\n",
    "x_train = np.array([[1, x[0], x[1]] for x in data_x])\n",
    "y_train = np.array(data_y)\n",
    "\n",
    "n_train = len(x_train)  # размер обучающей выборки\n",
    "w = [0.0, 0.0, 0.0]  # начальные весовые коэффициенты\n",
    "nt = np.array([0.1, 0.05, 0.05])  # шаг обучения для каждого параметра w0, w1, w2\n",
    "lm = 0.01  # значение параметра лямбда для вычисления скользящего экспоненциального среднего\n",
    "N = 200  # число итераций алгоритма SGD\n",
    "batch_size = 10 # размер мини-батча (величина K = 10)\n",
    "\n",
    "alpha = 0.7 # параметр для RMSProp\n",
    "G = np.zeros(len(w))  # параметр для RMSProp\n",
    "eps = 0.01 # параметр для RMSProp\n",
    "\n",
    "Qe = np.mean(loss(w, x_train.T, y_train))\n",
    "np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел\n",
    "\n",
    "for _ in range(N):\n",
    "    k = np.random.randint(0, n_train-batch_size-1) # n_train - размер выборки (массива x_train)\n",
    "\n",
    "    Qk = np.mean([loss(w, x_train[i], y_train[i]) for i in range(k,k+batch_size)])\n",
    "\n",
    "    dQk = np.mean([df(w, x_train[i], y_train[i]) for i in range(k, k+batch_size)], axis=0)\n",
    "\n",
    "    G = alpha * G + (1-alpha) * dQk * dQk\n",
    "\n",
    "    w -= nt * (dQk/(np.sqrt(G)+eps))\n",
    "\n",
    "    Qe = lm * Qk + (1-lm) * Qe\n",
    "\n",
    "Q = np.mean((np.dot(w, x_train.T) * y_train) < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd1c80-cf4a-4642-b315-2e2e73baaa83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
